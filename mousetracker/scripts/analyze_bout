#!/usr/bin/env python
"""
Bout Analyzer.   Extracts bilateral whisking and eyeblink data from a video snippet.

Usage:
    analyze_bout -h | --help
    analyze_bout --version
    analyze_bout ([-i <input_file> | --input <input_file>] | --print_config) [--config <config_file>]
                 [(-o <output_file> | --output <output_file>)] [(-v | --verbose)] [--clean]

Options:
    -h --help                   Show this screen and exit.
    --version                   Display the version and exit.
    --print_config              Print the default config value and exit.
    -i --input=<input_file>     Specify the file to process.
    -o --output=<output_file>   Specify a location to store the analyzed results.
    --config=<config_file>      Specify a path to a custom config file.  See --print-config for format.
    --clean                     If existing processed videos and analysis data exist, overwrite them with new.
    -v --verbose                Display extra diagnostic information during execution.

"""
import platform
import pprint
import shlex
import shutil
import subprocess
import sys
from logging import info, error, getLogger, ERROR
from os import access, W_OK, utime, remove, mkdir, path
from typing import List, Tuple, Optional, Any, Dict

import cv2
import pandas as pd
import progressbar
import attr
from attrs_utils.interop import from_docopt
from joblib import Parallel, delayed
import numpy as np

from mousetracker.core import eyes, yaml_config
from mousetracker.core._version import __version__
from mousetracker.core.base import RecordingSessionData, SideOfFace, VideoFileData
from mousetracker.core.whiskers import extract_whisk_data
from mousetracker.core.yaml_config import Config

KEEP_FILES = True


def main(inputargs: List[str]) -> int:
    """
    Application entry point.  Analyze a recorded bout based on the parameters laid out in a config file.
    :param inputargs:
    :return: A POSIX exit code.
    """
    args, app_config = __parse_args(inputargs=inputargs)

    side_files = split(args, app_config)
    for side in side_files:
        greyscale_and_extract_blink(side, app_config)
        realign(side,app_config)
        extract_whisk_data(side, app_config, KEEP_FILES)
    # info(f'processing file {path.split(args.input)[1]}')
    # eye_results = process_eyes(args, app_config)
    # info('Extracting whisk data for each eye')
    # Parallel(n_jobs=2)(delayed(extract_whisk_data)(f, app_config, KEEP_FILES) for f in eye_results.videos)
    # # for f in results.videos:
    # #     extract_whisk_data(f, app_config, KEEP_FILES)
    # analyze_bout(results=eye_results)
    return 0


def summarize_results(results: RecordingSessionData) -> None:
    """
    Combine left and right data into a summary data frame, and save it.
    :param results: 
    :return: 
    """
    # join left and right dataframes into one summary dataframe for the entire bout
    wholeface = pd.concat([pd.read_csv(f.summaryfile) for f in results.videos], axis=1)
    wholeface.to_csv(results.summarystats)

    # from mousetracker.core.analysis import make_summary_plots
    # info('Making summary plots...')
    # make_summary_plots(results)


def process_eyes(args: Any, app_config: Config) -> RecordingSessionData:
    """
    Break up a long recording into multiple small ones.  Collate split videos into one container per bout.
    :param args:
    :param app_config:
    :return:
    """
    name, ext = path.splitext(path.basename(args.input))

    left_file = path.join(args.output, name + "-left" + ext)
    right_file = path.join(args.output, name + "-right" + ext)

    return RecordingSessionData(videos=split_and_extract_blink(args, app_config, left_file, right_file))


def split(args: Any, app_config: Config) -> Tuple[VideoFileData, VideoFileData]:
    """
    Split a full-face recording into a left and right side.
    Uses FFMPEG and all avaialble cores in parallel.  
    """
    name, ext = path.splitext(path.basename(args.input))
    left_file = path.join(args.output, name + "-left" + ext)
    right_file = path.join(args.output, name + "-right" + ext)
    if not all(path.isfile(x) for x in (left_file, right_file)):
        ffmpegpath = shutil.which('ffmpeg')
        halfwidth = int(app_config.camera.width / 2)
        height = int(app_config.camera.height)
        right_box = f"{halfwidth}:{height}:0:0"
        left_box = f"{halfwidth}:{height}:{halfwidth}:0"
        commandstr = shlex.split(f'\"{ffmpegpath}\" -i \"{args.input}\" '
                                 f'-filter:v \"crop={left_box}\" -filter:a copy \"{left_file}\" '
                                 f'-filter:v \"crop={right_box}\" -filter:a copy \"{right_file}\" ')
        result = subprocess.run(commandstr, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if result.returncode != 0:
            raise IOError(f'ffmpeg failed to split file {args.input}!\n\t{result.stderr}')
    else:
        return (VideoFileData(name=left_file, side=SideOfFace.left),
               VideoFileData(name=right_file, side=SideOfFace.right))


def greyscale_and_extract_blink(filedata:VideoFileData, app_config: Config) -> None:
    """
    
    :param filepath: 
    :param side: 
    :param app_config: 
    :param outfile: 
    :return: 
    """
    # column_names = ['frameid', *[filedata.side.name + "_" + x for x in attr.asdict(eyes.EyeStats()).keys()]]
    # filedata.eye = pd.DataFrame(columns=column_names)
    if path.isfile(filedata.greyscale) and KEEP_FILES:
        info(f'Found existing file {filedata.greyscale}')
        return

    def extract_blink(curframe:int, frame:np.ndarray, side:SideOfFace):

        eyedata = eyes.compute_areas(frame)
        #eyedata_renamed = {side.name + "_" + k: v for k, v in eyedata.__dict__.items()}
        return {'frameid': curframe, **attr.asdict(eyedata)}

    def rename_blinks(blinks:List[Dict], side:SideOfFace) -> List[Dict]:
        def rename_key(key:str, side:SideOfFace) -> str:
            if key != "frameid":
                return f'{side.name}_{key}'
            else:
                return key
        retval = []
        for row in blinks:
            retval.append({rename_key(k,side): v for k, v in row.items()})
        return retval

    # grab the video
    cap = cv2.VideoCapture(filedata.name)
    size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
            int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))
    codec = cv2.VideoWriter_fourcc(*'MPEG')
    nframes = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    framerate = app_config.camera.framerate
    curframe = 0
    vw = cv2.VideoWriter(filename=filedata.greyscale, fourcc=codec, fps=framerate, frameSize=size,
                         isColor=False)

    with progressbar.ProgressBar(min_value=0, max_value=nframes) as pb:
        blinks = []
        while cap.isOpened():
            ret, frame = cap.read()
            if ret and (curframe < nframes):
                pb.update(curframe)
                # measure eye areas and store them in a dataframe
                blinks.append(extract_blink(curframe=curframe, frame=frame, side=filedata.side))
                # greyscale and invert for whisk detection
                frame = cv2.bitwise_not(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))
                # write out
                vw.write(frame)
                curframe += 1

            else:
                break
        # clean up, because openCV is stupid and doesn't implement `with ...`
        cap.release()
        vw.release()
        cv2.destroyAllWindows()
    renamed = rename_blinks(blinks, filedata.side)
    filedata.eye = pd.DataFrame(renamed)
    filedata.eye.to_csv(filedata.eyecheck)



def align_eyes(left: VideoFileData, right: VideoFileData) -> None:
    """
    Clean up eye data frames. Add a new column that scales the area by the largest value across both eyes.
    :param left: 
    :param right: 
    :return: 
    """
    left.eye = left.eye.set_index('frameid')
    right.eye = right.eye.set_index('frameid')

    left_fitted_ellipse = left.side.name + '_fitted_area'
    right_fitted_ellipse = right.side.name + '_fitted_area'

    max_left = max(abs(left.eye[left_fitted_ellipse]))
    max_right = max(abs(right.eye[right_fitted_ellipse]))
    scaling_factor = max(max_left, max_right)

    # add new column
    left.eye[left.side.name + '_scaled'] = (left.eye[left_fitted_ellipse] / scaling_factor) * 100

    # add new column
    right.eye[right.side.name + '_scaled'] = (right.eye[right_fitted_ellipse] / scaling_factor) * 100

    info('Saved eye data checkpoint file.')
    left.eye.to_csv(left.eyecheck)
    right.eye.to_csv(right.eyecheck)


def split_and_extract_blink(args, app_config, left_path, right_path):
    """
    Split a video into left and right face sides for whisking detection, and extract eye areas along the way.
    :param chunk:
    :param args:
    :param app_config:
    :return:
    """

    # grab the video
    cap = cv2.VideoCapture(args.input)
    # initialize storage containers
    nframes = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    left_column_names = ['frameid', *[SideOfFace.left.name + "_" + x for x in attr.asdict(eyes.EyeStats()).keys()]]
    right_column_names = ['frameid', *[SideOfFace.right.name + "_" + x for x in attr.asdict(eyes.EyeStats()).keys()]]
    left = VideoFileData(name=left_path, side=SideOfFace.left, eye=pd.DataFrame(columns=left_column_names),
                         nframes=nframes)
    right = VideoFileData(name=right_path, side=SideOfFace.right, eye=pd.DataFrame(columns=right_column_names),
                          nframes=nframes)

    # jump to the right frame
    # cap.set(1, 0)
    codec = cv2.VideoWriter_fourcc(*'MPEG')
    framerate = app_config.camera.framerate

    # get video dimensions
    size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
            int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))

    # compute dimensions of a vertical split
    cropped_size = (round(size[0] / 2), size[1])
    # open file handles for left and right videos
    if not (path.isfile(left.name) and path.isfile(right.name) and path.isfile(right.eyecheck) and path.isfile(
            left.eyecheck) and KEEP_FILES):
        info('Extracting left and right sides...')
        info('Detecting eye areas...')
        vw_left = cv2.VideoWriter(filename=left.name, fourcc=codec, fps=framerate, frameSize=cropped_size,
                                  isColor=False)
        vw_right = cv2.VideoWriter(filename=right.name, fourcc=codec, fps=framerate, frameSize=cropped_size,
                                   isColor=False)
        curframe = 0
        with progressbar.ProgressBar(min_value=0, max_value=nframes) as pb:
            while cap.isOpened():
                ret, frame = cap.read()
                if ret and (curframe < nframes):
                    pb.update(curframe)
                    # split in half
                    left_frame = frame[0:cropped_size[1], cropped_size[0]:size[0]]
                    right_frame = frame[0:cropped_size[1], 0:cropped_size[0]]
                    # measure eye areas
                    left_eye = eyes.compute_areas(left_frame)
                    left_eye_renamed = {SideOfFace.left.name + "_" + k: v for k, v in left_eye.__dict__.items()}
                    right_eye = eyes.compute_areas(right_frame)
                    right_eye_renamed = {SideOfFace.right.name + "_" + k: v for k, v in right_eye.__dict__.items()}
                    left.eye = left.eye.append({'frameid': curframe, **left_eye_renamed}, ignore_index=True)
                    right.eye = right.eye.append({'frameid': curframe, **right_eye_renamed}, ignore_index=True)
                    # greyscale and invert for whisk detection
                    left_frame = cv2.bitwise_not(cv2.cvtColor(left_frame, cv2.COLOR_BGR2GRAY))
                    right_frame = cv2.bitwise_not(cv2.cvtColor(right_frame, cv2.COLOR_BGR2GRAY))
                    # write out
                    vw_left.write(left_frame)
                    vw_right.write(right_frame)
                    curframe += 1
                    # uncomment to see live preview.
                    # cv2.imshow('left', left)
                    # cv2.imshow('right', right)
                    # if cv2.waitKey(1) & 0xFF == ord('q'):
                    #    break
                else:
                    break
            # clean up, because openCV is stupid and doesn't implement `with ...`
            cap.release()
            vw_left.release()
            vw_right.release()
            cv2.destroyAllWindows()

            # make checkpoint eye data
            align_eyes(left, right)
    else:
        info('Found existing split video.  Importing existing eye data checkpoint files.')
        left.eye = pd.read_csv(left.eyecheck)
        right.eye = pd.read_csv(right.eyecheck)
    # either return or die.
    if path.isfile(left.name) and path.isfile(right.name):
        aligned_l = __align_timestamps(left.name, args, app_config)
        aligned_r = __align_timestamps(right.name, args, app_config)
        if path.isfile(aligned_l) and path.isfile(aligned_r):
            left.name = aligned_l
            right.name = aligned_r
            info("wrote {0}".format(left.name))
            info("wrote {0}".format(right.name))
            return left, right
        else:
            raise IOError(f"Video pre-processing failed on file {args.input}")
    else:
        raise IOError(f"Video pre-processing failed on file {args.input}")


def realign(video:VideoFileData, app_config: Config) -> None:
    """
    reencode a video file with ffmpeg to align timestamps for whisker extraction.    
    """
    import shutil
    ffmpegpath = shutil.which('ffmpeg')

    if not (path.exists(video.aligned) and KEEP_FILES):
        info(f'aligning timestamps and creating {video.aligned}')
        if path.exists(video.aligned):
            remove(video.aligned)
        framerate = str(app_config.camera.framerate)
        command = [ffmpegpath, '-i', video.greyscale, '-codec:v', 'mpeg4', '-r', framerate,
                   '-qscale:v', '2', '-codec:a', 'copy', video.aligned]
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if result.returncode != 0:
            raise IOError(f'could not align {video.name}')
    else:
        info(f'found previously aligned timestamps for {video.aligned}')


def __align_timestamps(video: str, args: Any, app_config: Config) -> Optional[str]:
    """
    reencode a video file with ffmpeg to align timestamps for whisker extraction.
    :param video:
    :param args:
    :param app_config:
    :return:
    """
    import shutil
    ffmpegpath = shutil.which('ffmpeg')
    name, ext = path.splitext(video)
    aligned = name + "-aligned" + ext
    if not (path.exists(aligned) and KEEP_FILES):
        info(f'aligning timestamps and creating {aligned}')
        if path.exists(aligned):
            remove(aligned)
        framerate = str(app_config.camera.framerate)
        command = [ffmpegpath, '-i', video, '-codec:v', 'mpeg4', '-r', framerate,
                   '-qscale:v', '2', '-codec:a', 'copy', aligned]
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return aligned if result.returncode == 0 else None
    else:
        info(f'found previously aligned timestamps for {aligned}')
        return aligned


def __check_requirements() -> None:
    """
    This code relies on a variety of external tools to be available.  If they aren't, warn and barf.
    :return: diagnostic information about what's missing.
    """
    system = platform.system().casefold()
    if system == "windows":
        pass
    else:
        error(f"{system} is not supported (windows only for now)")
        sys.exit(1)
    if not shutil.which('ffmpeg'):
        error('ffmpeg is not installed or not on the system path!')
        sys.exit(1)
    if not all([shutil.which(x) for x in ('trace', 'classify', 'reclassify', 'measure')]):
        error('whisk tracking binaries are not installed or not on the system path!')
        sys.exit(1)


def __validate_args(args: Any, app_config: Config) -> None:
    """
    Makes sure the arguments passed in are reasonable.  Sets the default value for output, if required.  Configures
    logging level.
    :param args:
    :return:
    """
    if not path.isfile(args.input):
        raise FileNotFoundError(f'{args.input} not found!')
    else:
        if not args.output:
            videoname = path.splitext(path.basename(args.input))[0]
            args.output = path.join(path.split(args.input)[0], videoname)
            if not path.exists(args.output):
                mkdir(args.output)
                __touch(path.join(args.output, app_config.storage.root_label))
    if not access(args.output, W_OK):
        raise PermissionError(f'{args.output} is not writable!')
    else:
        if not path.exists(path.join(args.output, app_config.storage.root_label)):
            __touch(path.join(args.output, app_config.storage.root_label))
    if not args.verbose:
        getLogger().setLevel(ERROR)
    if args.config and not path.isfile(args.config):
        raise FileNotFoundError(f'User-supplied configuration file {args.config} not found!')


def __touch(fname: str, times: Tuple[float, float] = None):
    """ As coreutils touch.
    """
    with open(fname, 'a+'):
        utime(fname, times)


def __parse_args(inputargs: List[str]) -> Tuple[Any, Config]:
    args = from_docopt(docstring=__doc__, argv=inputargs, version=__version__)
    __check_requirements()
    info('read default hardware parameters.')
    app_config = yaml_config.load(args.config)
    if args.print_config:
        print("Detected Configuration Parameters: ")
        pprint.pprint(attr.asdict(app_config), depth=5)
        sys.exit(0)
    __validate_args(args, app_config)
    if args.clean:
        global KEEP_FILES  # ew.
        KEEP_FILES = False
    return args, app_config


if __name__ == "__main__":
    # guarantee that we pass the right number of arguments.
    sys.exit(main(sys.argv[1:] if len(sys.argv) > 1 else ["-h"]))
